
=== TODOs.  Last updated 17 Mar 2020. =====================================

=== Tidying ===============================================================

- (should do for MVP): fn CFGInfo::create::dfs: use an explicit stack instead
  of recursion.  As it stands, we could get a stack overflow if given a fn
  with several thousand BBs, all in a chain.

=== Interface =============================================================

- Require clients to specify the number of virtual regs in the input, so that
  we can use DenseSet<VirtualReg>.

- Allow clients to specify an is_leaf_function flag, so we can switch
  allocation ordering to favour leaf functions in such cases.

=== Allocator run time (analysis phase, algorithmic) ======================

Estimated most-important-first:

- merge_RangeFrags: try to avoid quadratic behaviour for large merge-sets

- Dominator computation: switch to "A Simple, Fast Dominance Algorithm (Keith
  D. Cooper, Timothy J. Harvey, and Ken Kennedy)" and reimplement the
  loop-finder on top of that.

- Iterative liveness analysis: only put blocks in the work queue if they are
  not already present in it.  This could reduce the number of evaluations by
  quite a lot.  Requires a working DenseSet<BlockIx>.

- (Longer term) merge_RangeFrags: can we do even better?

- (Longer term) Maybe remove dominance computation and the existing loop
  finder completely, and instead use a standalone loop-finder algorithm, eg
  https://lenx.100871.net/papers/loop-SAS.pdf, as suggested by Chris.

=== Allocator run time (both BT and LSRA, algorithmic) ====================

- Remove scan at end of allocation that figures out which real regs got used.
  Instead harvest that information from the individual algorithm's rreg-state
  tables at the end of allocation.  Doing this will remove a pass over all
  instructions so it seems like a significant win.

- Inst rewrite loop: move cursors forwards at Point granularity so we don't
  have to repeatedly re-scan the groups looking for particular LR kinds?

=== Allocator run time (LSRA, algorithmic) ================================

=== Allocator run time (BT, algorithmic) ==================================

- Inst rewrite loop: don't clone map_defs; just use it as soon as it's
  available.

=== Allocator run time (representational) =================================

- Collect typical use data for each Set<T> instance and replace with a
  suitable optimised replacement.  Maybe DenseSet<T: ToFromU32> and
  SparseSet<T: ToFromU32>.

- Ditto FxHashMap (if we have to have it at all)

- Replace SortedFragIxs with something more efficient.  Does it even need to
  be sorted?  Maybe it should be changed to a SparseSet<RangeFragIx>.

- Profile with DHAT and selectively change Vec to SmallVec as needed.  Do not
  change Vec to SmallVec without supporting DHAT profile data; this can make
  things run more slowly.

- (Longer term) investigate the feasibility of removing the fragment
  environment and instead storing fragment range data directly in what is
  currently RangeFragIx.

=== Functionality =====================================================

MVP (without these, the implementation is useless in practice):

- add a spill-slot allocation mechanism, even if it is pretty crude.  At
  least for BT, organise this so that it supports coalescing of moves
  between spill slots.  LSRA should ideally support that too.

Post-MVP:

- BT: Live Range Splitting (but see next point)

- BT: rewrite the main live-range allocation loop, so as
  to make it perform copy coalescing more robustly.

  Currently the loop processes one VirtualRange at a time, and makes a
  best-effort to honour the coalescing hints available.  However, this is all
  rather ad hoc.  It would be better to change the structure so that the basic
  unit of allocation is a *group* of VirtualRanges, where VirtualRanges are
  put into the same group if they are connected by V-V copies.  Hence:

  * coalescing analysis computes:
    - for each VLR, a weighted set of register prefs, based on V-R and R-V
      copies only
    - the grouping of VLRs, based on V-V copies

  * the allocation loop then allocates/evicts entire groups at a time.
    Allocating an entire group at once guarantees that all V-V copies between
    VLRs in the group will disappear, since by definition all group members
    have the same real register.

  * when allocating a group, draw up a weighted list of preferred candidates
    by checking the V-R/R-V derived prefs of all its members, and try first to
    use those (as the present logic does).

  * if a group can't be allocated, and contains more than one VLR, then it is
    split up into multiple groups, each containing one VLR, and these are put
    back into the allocation queue.  Note that this doesn't have to happen in
    a single step; it would be ok to gradually split the groups into smaller
    groups, with the proviso that the eventual end-point of the process must
    result in each group containing only a single VLR.

    The result of splitting up a group is that we can no longer guarantee to
    remove V-V copies between the group's VLRs, but that's unavoidable.

  * if a group can't be allocated, and contains only a single VLR, then either
    - it must be spilled, as at present
    - it must be split into multiple smaller VLRs, each of which is put into
      its own group. (Not yet implemented).

  This design has some nice properties:

  * It maximally coalesces virtual groups to the extent it can.

  * Imagine the situation where a group has a real-reg preference, as a result
    of one of its members (VLRs) having that preference.  Suppose further that
    the group cannot be allocated, and must be split, so it is split into two
    smaller groups.  The group that does *not* inherit the preferred register
    is now unconstrained by the preference, and the group that does inherit
    the preference is smaller.  Both of these effects potentially remove
    interferences and so increase the chances of the two smaller groups being
    successfully allocated.

  * It integrates nicely with the idea of splitting.  If a single-element
    group (a VLR) cannot be allocated, we could choose to split it into
    multiple single-element groups.  The only difficulty is to decide how to
    place the splits in such a way that it is unnecessary to add explicit
    copies to the instruction stream (since that sounds complex and expensive)
    and yet is profitable.

    The no-changes-to-the-insn stream idea may be a tradeoff that is worth
    persuing in an allocator intended for use in a JIT.  It reduces the
    splitter's possibilities but means it doesn't have to deal with the
    complexity of adding copies at the dominance frontiers, etc.  If done
    carefully it could mean we never need to change the instruction stream at
    all.  The restriction basically has the effect that any split point must
    divide the blocks in the VLR into two disjoint groups, those dominated by
    the "split point" and those postdominated by the "split point", so that in
    effect all "traffic" in the VLR flows through the split point.

    Another opportunistic-split possibility is to add two splits at the start
    and end of a single basic block that has high register pressure, perhaps
    due to a call.  Then we wouldn't even have to bother checking the
    dominates/postdominates condition.

    These are complex tradeoffs to evaluate.  Perhaps in 2H2020.

=== end ===================================================================
